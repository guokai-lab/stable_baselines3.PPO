Proximal Policy Optimization algorithm (PPO) (clip version)

Paper: https://arxiv.org/abs/1707.06347
Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and
Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)

Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html

policy: The policy model to use (MlpPolicy, CnnPolicy, MultiInputPolicy)
env: The environment to learn from (if registered in Gym, can be str)
learning_rate: The learning rate, it can be a function of the current progress remaining (from 1 to 0)
n_steps: The number of steps to run for each environment per update
        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)
        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)
        See https://github.com/pytorch/pytorch/issues/29372
batch_size: Minibatch size
n_epochs: Number of epoch when optimizing the surrogate loss
gamma: Discount factor
gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator
clip_range: Clipping parameter, it can be a function of the current progress remaining (from 1 to 0).
clip_range_vf: Clipping parameter for the value function,
        it can be a function of the current progress remaining (from 1 to 0).
        This is a parameter specific to the OpenAI implementation. If None is passed (default),
        no clipping will be done on the value function.
        IMPORTANT: this clipping depends on the reward scaling.
normalize_advantage: Whether to normalize or not the advantage
ent_coef: Entropy coefficient for the loss calculation
vf_coef: Value function coefficient for the loss calculation
max_grad_norm: The maximum value for the gradient clipping
use_sde: Whether to use generalized State Dependent Exploration (gSDE)
        instead of action noise exploration (default: False)
sde_sample_freq: Sample a new noise matrix every n steps when using gSDE
        Default: -1 (only sample at the beginning of the rollout)
rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.
rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation
target_kl: Limit the KL divergence between updates,
        because the clipping is not enough to prevent large update
        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)
        By default, there is no limit on the kl div.
stats_window_size: Window size for the rollout logging, specifying the number of episodes to average
        the reported success rate, mean episode length, and mean reward over
tensorboard_log: the log location for tensorboard (if None, no logging)
policy_kwargs: additional arguments to be passed to the policy on creation. See :ref:`ppo_policies`
verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for
        debug messages
seed: Seed for the pseudo random generators
device: Device (cpu, cuda, ...) on which the code should be run.
        Setting it to auto, the code will be run on the GPU if possible.
_init_setup_model: Whether or not to build the network at the creation of the instance
