Proximal Policy Optimization algorithm (PPO) (clip version)

Paper: https://arxiv.org/abs/1707.06347
Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and
Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)

Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html

policy: 指定使用的策略网络包括 (MlpPolicy, CnnPolicy, MultiInputPolicy)
env: 强化学习环境 (Gym环境或自定义)
learning_rate: 学习率（步长），控制模型更新速度
n_steps: 单个环境每次更新收集的步数，较大的步数可以提供更多信息但会增加计算量
batch_size: 单次更新所需样本数
n_epochs: Number of epoch when optimizing the surrogate loss
gamma: Discount factor
gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator
clip_range: Clipping parameter, it can be a function of the current progress remaining (from 1 to 0).
clip_range_vf: Clipping parameter for the value function,
        it can be a function of the current progress remaining (from 1 to 0).
        This is a parameter specific to the OpenAI implementation. If None is passed (default),
        no clipping will be done on the value function.
        IMPORTANT: this clipping depends on the reward scaling.
normalize_advantage: Whether to normalize or not the advantage
ent_coef: Entropy coefficient for the loss calculation
vf_coef: Value function coefficient for the loss calculation
max_grad_norm: The maximum value for the gradient clipping
use_sde: Whether to use generalized State Dependent Exploration (gSDE)
        instead of action noise exploration (default: False)
sde_sample_freq: Sample a new noise matrix every n steps when using gSDE
        Default: -1 (only sample at the beginning of the rollout)
rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.
rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation
target_kl: Limit the KL divergence between updates,
        because the clipping is not enough to prevent large update
        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)
        By default, there is no limit on the kl div.
stats_window_size: Window size for the rollout logging, specifying the number of episodes to average
        the reported success rate, mean episode length, and mean reward over
tensorboard_log: the log location for tensorboard (if None, no logging)
policy_kwargs: additional arguments to be passed to the policy on creation. See :ref:`ppo_policies`
verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for
        debug messages
seed: Seed for the pseudo random generators
device: Device (cpu, cuda, ...) on which the code should be run.
        Setting it to auto, the code will be run on the GPU if possible.
_init_setup_model: Whether or not to build the network at the creation of the instance
